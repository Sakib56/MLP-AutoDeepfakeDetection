{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Minimally Trained Baseline Tests","provenance":[{"file_id":"1UQUnqyHrzSZJr9H0Vi2mWUh-iQhbgTtv","timestamp":1617123690298}],"collapsed_sections":[],"authorship_tag":"ABX9TyNNR2exs5Efhgzl/bi5s7Bs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"K7DxFXyRUNPl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617232546720,"user_tz":-60,"elapsed":38110,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"9d889f16-c93a-4e8c-8766-15eb7605167d"},"source":["# For Google Colab use\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    %cd '/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder'    \n","except ModuleNotFoundError:\n","    pass"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7W8i8xxtURw5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1617232584267,"user_tz":-60,"elapsed":75650,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"ee4b344a-0def-42a4-90a3-ae5fb3d0a42e"},"source":["# Imports\n","from __future__ import division\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","from numpy.random import seed\n","\n","import tensorflow as tf\n","\n","import keras\n","from keras import preprocessing\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import layers, Model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","from keras.optimizers import *\n","from keras.applications import *\n","from keras import metrics\n","from keras.losses import binary_crossentropy\n","from keras import backend as K\n","\n","try:\n","    import face_recognition\n","except ModuleNotFoundError:\n","    !pip install face_recognition\n","\n","from Util import pipeline\n","from Baseline.classifiers import *"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting face_recognition\n","  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Collecting face-recognition-models>=0.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n","\u001b[K     |████████████████████████████████| 100.2MB 31kB/s \n","\u001b[?25hRequirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n","Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n","Building wheels for collected packages: face-recognition-models\n","  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=be69e0985a8dcb696934536e537dc933617bae700cc0c2d1759a1986d5955419\n","  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n","Successfully built face-recognition-models\n","Installing collected packages: face-recognition-models, face-recognition\n","Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"F6iN8C_aMd6Q","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1857d407-d169-4a3a-8b9a-e99d75159746"},"source":["# General model settings\n","IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 256, 256, 3\n","EPOCHS = 100\n","DATA_GENERATOR_SEED = 1337\n","BATCH_SIZE = 128\n","VALIDATION_SPLIT = 0.2\n","\n","tf.random.set_seed(DATA_GENERATOR_SEED)\n","seed(DATA_GENERATOR_SEED)\n","\n","# Pick dataset; DF_TYPE={'rnd', 'avg'}\n","for DF_TYPE in ['diff', 'avg', 'rnd']:\n","    TRAIN_VAL_DIR = f'./Celeb-DF-v2/Celeb-{DF_TYPE}-30-test'\n","\n","    TRAIN_DATAGEN = ImageDataGenerator(rescale = 1.0/255.0, horizontal_flip = True, fill_mode='nearest', validation_split = VALIDATION_SPLIT)\n","    TRAIN_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n","                                                        batch_size = BATCH_SIZE,\n","                                                        class_mode = 'binary', \n","                                                        target_size = (IMG_HEIGHT, IMG_WIDTH),\n","                                                        subset = 'training',\n","                                                        seed = DATA_GENERATOR_SEED,\n","                                                        follow_links = True)\n","\n","    VAL_DATAGEN = ImageDataGenerator(rescale = 1.0/255.0, validation_split = VALIDATION_SPLIT)\n","    VALIDATION_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n","                                                             batch_size = BATCH_SIZE,\n","                                                             class_mode = 'binary', \n","                                                             target_size = (IMG_HEIGHT, IMG_WIDTH),\n","                                                             subset = 'validation',\n","                                                             seed = DATA_GENERATOR_SEED,\n","                                                             follow_links = True)\n","\n","    # Train MesoInception4 w/ F2F weights\n","    TEST_MODEL = MesoInception4().model\n","    TEST_MODEL.compile(optimizer = TEST_MODEL.optimizer,\n","                       loss = TEST_MODEL.loss,\n","                       metrics = TEST_MODEL.metrics + [metrics.BinaryAccuracy(name = 'acc'),\n","                                                       metrics.AUC(name = 'auc'),\n","                                                       metrics.FalsePositives(name = 'fp')])\n","    \n","    WEIGHTS_PATH = './Baseline/weights/MesoInception_F2F.h5'\n","    TEST_MODEL.load_weights(WEIGHTS_PATH) \n","\n","    # Make last layer trainable\n","    for layer in TEST_MODEL.layers:\n","        layer.trainable = False\n","    TEST_MODEL.layers[-1].trainable = True\n","\n","    train_gen_list = list(TRAIN_GENERATOR.classes)\n","    val_gen_list = list(VALIDATION_GENERATOR.classes)\n","\n","    train_neg, train_pos = train_gen_list.count(0), train_gen_list.count(1)\n","    val_neg, val_pos = val_gen_list.count(0), val_gen_list.count(1)\n","\n","    pos = train_pos + val_pos\n","    neg = train_neg + val_neg\n","    total = pos + neg\n","\n","    weight_for_0 = (1 / neg)*(total)/2.0 \n","    weight_for_1 = (1 / pos)*(total)/2.0\n","\n","    CLASS_WEIGHT = {0: weight_for_0, 1: weight_for_1}\n","\n","    EARLY_STOP = EarlyStopping(monitor='val_auc',\n","                            patience=EPOCHS//20,\n","                            mode='max',\n","                            verbose=1,\n","                            restore_best_weights=True)\n","\n","    HISTORY = TEST_MODEL.fit(TRAIN_GENERATOR,\n","                            steps_per_epoch = TRAIN_GENERATOR.n//TRAIN_GENERATOR.batch_size,\n","                            validation_data = VALIDATION_GENERATOR,\n","                            validation_steps = VALIDATION_GENERATOR.n//VALIDATION_GENERATOR.batch_size,\n","                            epochs = EPOCHS,\n","                            verbose = 1,\n","                            class_weight = CLASS_WEIGHT,\n","                            callbacks = [EARLY_STOP])\n","\n","    weight_name = WEIGHTS_PATH.split('_')[-1].replace('.h5', '')\n","    pipeline.evaluate_model(TEST_MODEL = TEST_MODEL,\n","                            EXPERIMENT_NAME = f'Minimally Trained MesoInception4 on {DF_TYPE} with {weight_name}',\n","                            TEST_DIR = f'./Celeb-DF-v2/Celeb-{DF_TYPE}-30-test',\n","                            WEIGHTS_PATH = '',\n","                            HISTORY = HISTORY)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Found 15711 images belonging to 2 classes.\n","Found 3927 images belonging to 2 classes.\n","Epoch 1/100\n","122/122 [==============================] - 5227s 43s/step - loss: 0.3311 - acc: 0.4861 - auc: 0.4884 - fp: 302.5366 - val_loss: 0.1020 - val_acc: 0.8943 - val_auc: 0.7332 - val_fp: 222.0000\n","Epoch 2/100\n","122/122 [==============================] - 76s 622ms/step - loss: 0.2005 - acc: 0.7124 - auc: 0.7604 - fp: 207.5366 - val_loss: 0.2820 - val_acc: 0.5810 - val_auc: 0.7603 - val_fp: 65.0000\n","Epoch 3/100\n","122/122 [==============================] - 76s 618ms/step - loss: 0.1638 - acc: 0.7781 - auc: 0.8407 - fp: 164.2602 - val_loss: 0.4816 - val_acc: 0.3753 - val_auc: 0.6886 - val_fp: 35.0000\n","Epoch 4/100\n","122/122 [==============================] - 75s 616ms/step - loss: 0.1528 - acc: 0.7652 - auc: 0.8633 - fp: 141.3496 - val_loss: 0.2278 - val_acc: 0.6625 - val_auc: 0.8083 - val_fp: 70.0000\n","Epoch 5/100\n","122/122 [==============================] - 76s 618ms/step - loss: 0.1298 - acc: 0.8169 - auc: 0.8994 - fp: 118.4390 - val_loss: 0.4429 - val_acc: 0.4182 - val_auc: 0.7742 - val_fp: 32.0000\n","Epoch 6/100\n","122/122 [==============================] - 76s 619ms/step - loss: 0.1170 - acc: 0.8300 - auc: 0.9159 - fp: 102.1057 - val_loss: 0.5692 - val_acc: 0.2596 - val_auc: 0.7407 - val_fp: 17.0000\n","Epoch 7/100\n","122/122 [==============================] - 75s 616ms/step - loss: 0.1059 - acc: 0.8458 - auc: 0.9259 - fp: 80.0407 - val_loss: 0.4157 - val_acc: 0.4536 - val_auc: 0.7960 - val_fp: 22.0000\n","Epoch 8/100\n","122/122 [==============================] - 76s 624ms/step - loss: 0.0990 - acc: 0.8517 - auc: 0.9347 - fp: 77.4146 - val_loss: 0.2920 - val_acc: 0.6010 - val_auc: 0.8104 - val_fp: 48.0000\n","Epoch 9/100\n","122/122 [==============================] - 76s 622ms/step - loss: 0.0949 - acc: 0.8596 - auc: 0.9398 - fp: 72.0325 - val_loss: 0.3111 - val_acc: 0.5555 - val_auc: 0.8203 - val_fp: 41.0000\n","Epoch 10/100\n","122/122 [==============================] - 75s 613ms/step - loss: 0.0862 - acc: 0.8748 - auc: 0.9509 - fp: 64.5203 - val_loss: 0.7469 - val_acc: 0.1159 - val_auc: 0.7191 - val_fp: 2.0000\n","Epoch 11/100\n","122/122 [==============================] - 75s 614ms/step - loss: 0.0862 - acc: 0.8763 - auc: 0.9468 - fp: 64.6911 - val_loss: 0.3387 - val_acc: 0.5594 - val_auc: 0.8046 - val_fp: 45.0000\n","Epoch 12/100\n","122/122 [==============================] - 75s 614ms/step - loss: 0.0764 - acc: 0.8909 - auc: 0.9582 - fp: 62.4553 - val_loss: 0.3708 - val_acc: 0.5042 - val_auc: 0.8157 - val_fp: 24.0000\n","Epoch 13/100\n","122/122 [==============================] - 75s 614ms/step - loss: 0.0725 - acc: 0.8868 - auc: 0.9628 - fp: 52.5366 - val_loss: 0.3875 - val_acc: 0.5013 - val_auc: 0.8130 - val_fp: 28.0000\n","Epoch 14/100\n","122/122 [==============================] - 75s 616ms/step - loss: 0.0715 - acc: 0.8875 - auc: 0.9623 - fp: 44.7236 - val_loss: 0.3469 - val_acc: 0.5417 - val_auc: 0.7441 - val_fp: 69.0000\n","Restoring model weights from the end of the best epoch.\n","Epoch 00014: early stopping\n","Found 19638 images belonging to 2 classes.\n","307/307 [==============================] - 70s 225ms/step - loss: 0.1426 - acc: 0.7863 - auc: 0.9195 - fp: 163.0000\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n","  _warn_prf(average, modifier, msg_start, len(result))\n"],"name":"stderr"},{"output_type":"stream","text":["Found 1280 images belonging to 2 classes.\n","Found 3927 images belonging to 2 classes.\n","Epoch 1/100\n","10/10 [==============================] - 1461s 157s/step - loss: 0.5064 - acc: 0.5818 - auc: 0.0000e+00 - fp: 249.4545 - val_loss: 0.7255 - val_acc: 0.0854 - val_auc: 0.4309 - val_fp: 2.0000\n","Epoch 2/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0098 - acc: 0.9993 - auc: 0.0000e+00 - fp: 0.6364 - val_loss: 0.8783 - val_acc: 0.0805 - val_auc: 0.4411 - val_fp: 0.0000e+00\n","Epoch 3/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0042 - acc: 0.9976 - auc: 0.0000e+00 - fp: 1.2727 - val_loss: 0.9019 - val_acc: 0.0810 - val_auc: 0.4485 - val_fp: 0.0000e+00\n","Epoch 4/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0010 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9082 - val_acc: 0.0815 - val_auc: 0.4515 - val_fp: 0.0000e+00\n","Epoch 5/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0020 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9119 - val_acc: 0.0820 - val_auc: 0.4805 - val_fp: 0.0000e+00\n","Epoch 6/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0014 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9142 - val_acc: 0.0820 - val_auc: 0.4862 - val_fp: 0.0000e+00\n","Epoch 7/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0018 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9153 - val_acc: 0.0815 - val_auc: 0.4958 - val_fp: 0.0000e+00\n","Epoch 8/100\n","10/10 [==============================] - 19s 2s/step - loss: 7.0878e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9163 - val_acc: 0.0812 - val_auc: 0.4808 - val_fp: 0.0000e+00\n","Epoch 9/100\n","10/10 [==============================] - 19s 2s/step - loss: 8.1071e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9170 - val_acc: 0.0807 - val_auc: 0.4918 - val_fp: 0.0000e+00\n","Epoch 10/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0011 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9163 - val_acc: 0.0818 - val_auc: 0.4962 - val_fp: 0.0000e+00\n","Epoch 11/100\n","10/10 [==============================] - 19s 2s/step - loss: 5.1848e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9167 - val_acc: 0.0818 - val_auc: 0.4979 - val_fp: 0.0000e+00\n","Epoch 12/100\n","10/10 [==============================] - 19s 2s/step - loss: 8.0061e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9169 - val_acc: 0.0820 - val_auc: 0.4931 - val_fp: 0.0000e+00\n","Epoch 13/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0017 - acc: 0.9995 - auc: 0.0000e+00 - fp: 0.5455 - val_loss: 0.9180 - val_acc: 0.0812 - val_auc: 0.4949 - val_fp: 0.0000e+00\n","Epoch 14/100\n","10/10 [==============================] - 18s 2s/step - loss: 0.0018 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9178 - val_acc: 0.0818 - val_auc: 0.4961 - val_fp: 0.0000e+00\n","Epoch 15/100\n","10/10 [==============================] - 19s 2s/step - loss: 7.0310e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9182 - val_acc: 0.0815 - val_auc: 0.4976 - val_fp: 0.0000e+00\n","Epoch 16/100\n","10/10 [==============================] - 19s 2s/step - loss: 0.0011 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9190 - val_acc: 0.0807 - val_auc: 0.4986 - val_fp: 0.0000e+00\n","Epoch 17/100\n","10/10 [==============================] - 19s 2s/step - loss: 4.4373e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9185 - val_acc: 0.0812 - val_auc: 0.5000 - val_fp: 0.0000e+00\n","Epoch 18/100\n","10/10 [==============================] - 19s 2s/step - loss: 4.3303e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9185 - val_acc: 0.0812 - val_auc: 0.4966 - val_fp: 0.0000e+00\n","Epoch 19/100\n","10/10 [==============================] - 19s 2s/step - loss: 4.6282e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9177 - val_acc: 0.0820 - val_auc: 0.4967 - val_fp: 0.0000e+00\n","Epoch 20/100\n","10/10 [==============================] - 19s 2s/step - loss: 7.2783e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9180 - val_acc: 0.0818 - val_auc: 0.4969 - val_fp: 0.0000e+00\n","Epoch 21/100\n","10/10 [==============================] - 19s 2s/step - loss: 2.5415e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9172 - val_acc: 0.0826 - val_auc: 0.4966 - val_fp: 0.0000e+00\n","Epoch 22/100\n","10/10 [==============================] - 19s 2s/step - loss: 5.8606e-04 - acc: 1.0000 - auc: 0.0000e+00 - fp: 0.0000e+00 - val_loss: 0.9196 - val_acc: 0.0802 - val_auc: 0.4998 - val_fp: 0.0000e+00\n","Restoring model weights from the end of the best epoch.\n","Epoch 00022: early stopping\n","Found 19638 images belonging to 2 classes.\n","163/307 [==============>...............] - ETA: 29:08 - loss: 0.9202 - acc: 0.0797 - auc: 0.4990 - fp: 0.0000e+00"],"name":"stdout"}]}]}