{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"},"colab":{"name":"OC-FakeDect VAE Remake","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gU6tDeTCASMt","executionInfo":{"status":"ok","timestamp":1616107401040,"user_tz":0,"elapsed":1416,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"627ae981-0c02-4c68-8f86-eaf4ea467b13"},"source":["# For Google Colab use\n","try:\n","    from google.colab import drive\n","    drive.mount('/content/drive', force_remount=True)\n","    %cd '/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder'    \n","except ModuleNotFoundError:\n","    pass"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n","/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lqetwe06ASM7","executionInfo":{"status":"ok","timestamp":1616107402760,"user_tz":0,"elapsed":3123,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}}},"source":["# Imports\n","from __future__ import division\n","\n","import os\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n","from numpy.random import seed\n","\n","import tensorflow as tf\n","\n","import keras\n","from keras import preprocessing\n","from keras.preprocessing.image import ImageDataGenerator\n","from keras import layers, Model\n","from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n","from keras.optimizers import *\n","from keras.applications import *\n","from keras import metrics\n","from keras.losses import binary_crossentropy\n","from keras import backend as K\n","\n","# !pip install -U keras-tuner\n","# from kerastuner.tuners import RandomSearch, Hyperband\n","# from kerastuner.engine.hypermodel import HyperModel\n","# from kerastuner.engine.hyperparameters import HyperParameters\n","# from kerastuner import Objective"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Dqaubd73ASM-","executionInfo":{"status":"ok","timestamp":1616107403459,"user_tz":0,"elapsed":3815,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"cb79586e-bfa0-43e8-a50e-46f90abca096"},"source":["# Check GPU available\n","%tensorflow_version 2.x\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Found GPU at: /device:GPU:0\n","Num GPUs Available:  1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZGWBPcE9ASNM","executionInfo":{"status":"ok","timestamp":1616107403461,"user_tz":0,"elapsed":3810,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"dba64e69-898e-4b03-c02f-96f2e0b93b2e"},"source":["# General model settings\n","IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 100, 100, 3\n","EPOCHS = 1\n","DATA_GENERATOR_SEED = 1337\n","BATCH_SIZE = 192\n","VALIDATION_SPLIT = 0.1\n","LATENT_DIM = 20000\n","tf.random.set_seed(DATA_GENERATOR_SEED)\n","seed(DATA_GENERATOR_SEED)\n","\n","# Pick dataset; DF_TYPE={'rnd-30', 'avg-30'}\n","DF_TYPE = 'rnd-30'\n","TRAIN_VAL_DIR = f'./Celeb-DF-v2/Celeb-{DF_TYPE}'\n","TEST_DIR = f'{TRAIN_VAL_DIR}-test'\n","\n","TRAIN_VAL_DIR += '/Celeb-real/'\n","\n","TRAIN_VAL_DIR = './Celeb-DF-v2/Celeb-TEMP'\n","# TEST_DIR += '/Celeb-real'\n","TRAIN_VAL_DIR, TEST_DIR"],"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["('./Celeb-DF-v2/Celeb-TEMP', './Celeb-DF-v2/Celeb-rnd-30-test')"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"5uU-cdzia3fW","executionInfo":{"status":"ok","timestamp":1616107403463,"user_tz":0,"elapsed":3803,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}}},"source":["# !ln -s \"/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder/Celeb-DF-v2/Celeb-avg-30/Celeb-real\" \"/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder/Celeb-DF-v2/Celeb-TEMP/sym\"\n","# !ls \"/content/drive/MyDrive/Colab Notebooks/MLP-DeepfakeDetection-VariationalAutoencoder/Celeb-DF-v2/Celeb-TEMP\""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OkC6WEcdASNV","executionInfo":{"status":"ok","timestamp":1616107403652,"user_tz":0,"elapsed":3985,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"f57194a6-f0dd-4148-db2f-37fea6dafcc4"},"source":["TRAIN_DATAGEN = ImageDataGenerator(rescale = 1.0/255.0, horizontal_flip = True, fill_mode='nearest', validation_split = VALIDATION_SPLIT)\n","TRAIN_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n","                                                    batch_size = BATCH_SIZE,\n","                                                    class_mode = 'input', \n","                                                    target_size = (IMG_HEIGHT, IMG_WIDTH),\n","                                                    subset = 'training',\n","                                                    seed = DATA_GENERATOR_SEED,\n","                                                    follow_links = True)\n","\n","VAL_DATAGEN = ImageDataGenerator(rescale = 1.0/255.0, validation_split = VALIDATION_SPLIT)\n","VALIDATION_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n","                                                         batch_size = BATCH_SIZE,\n","                                                         class_mode = 'input', \n","                                                         target_size = (IMG_HEIGHT, IMG_WIDTH),\n","                                                         subset = 'validation',\n","                                                         seed = DATA_GENERATOR_SEED)\n","\n","# TEST_DATAGEN = ImageDataGenerator(rescale = 1.0/255.0)\n","# TEST_GENERATOR = TEST_DATAGEN.flow_from_directory(directory = TEST_DIR,\n","#                                                   batch_size = BATCH_SIZE,\n","#                                                   class_mode = 'binary', \n","#                                                   target_size = (IMG_HEIGHT, IMG_WIDTH),                                \n","#                                                   seed = DATA_GENERATOR_SEED)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Found 5065 images belonging to 1 classes.\n","Found 562 images belonging to 1 classes.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"2ZTdI0O3AfQO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616107404116,"user_tz":0,"elapsed":4439,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"098c4c4c-4a90-4ead-a588-aaa883c64d4f"},"source":["# OC-FakeDect Encoder\n","\n","encoder_inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n","# 100x100x3\n","x = layers.Conv2D(16, 3, strides=1, padding=\"same\")(encoder_inputs)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 100x100x16\n","x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 50x50x32\n","x = layers.Conv2D(64, 3, strides=1, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 50x50x64\n","x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 25x25x32\n","x = layers.Flatten()(x)\n","# 20000\n","\n","# Sampling Class, Random sampling function for latent vector, z; \"variational part\"\n","class Sampling(layers.Layer):\n","    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n","    def call(self, inputs):\n","        z_mean, z_log_var = inputs\n","        batch = tf.shape(z_mean)[0]\n","        dim = tf.shape(z_mean)[1]\n","        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n","        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n","# Sampling\n","z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n","z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n","z = Sampling()([z_mean, z_log_var])\n","\n","# Defining full encoder as keras model\n","encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","encoder.summary()"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Model: \"encoder\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_1 (InputLayer)            [(None, 100, 100, 3) 0                                            \n","__________________________________________________________________________________________________\n","conv2d (Conv2D)                 (None, 100, 100, 16) 448         input_1[0][0]                    \n","__________________________________________________________________________________________________\n","batch_normalization (BatchNorma (None, 100, 100, 16) 400         conv2d[0][0]                     \n","__________________________________________________________________________________________________\n","activation (Activation)         (None, 100, 100, 16) 0           batch_normalization[0][0]        \n","__________________________________________________________________________________________________\n","conv2d_1 (Conv2D)               (None, 50, 50, 32)   4640        activation[0][0]                 \n","__________________________________________________________________________________________________\n","batch_normalization_1 (BatchNor (None, 50, 50, 32)   200         conv2d_1[0][0]                   \n","__________________________________________________________________________________________________\n","activation_1 (Activation)       (None, 50, 50, 32)   0           batch_normalization_1[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_2 (Conv2D)               (None, 50, 50, 64)   18496       activation_1[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_2 (BatchNor (None, 50, 50, 64)   200         conv2d_2[0][0]                   \n","__________________________________________________________________________________________________\n","activation_2 (Activation)       (None, 50, 50, 64)   0           batch_normalization_2[0][0]      \n","__________________________________________________________________________________________________\n","conv2d_3 (Conv2D)               (None, 25, 25, 32)   18464       activation_2[0][0]               \n","__________________________________________________________________________________________________\n","batch_normalization_3 (BatchNor (None, 25, 25, 32)   100         conv2d_3[0][0]                   \n","__________________________________________________________________________________________________\n","activation_3 (Activation)       (None, 25, 25, 32)   0           batch_normalization_3[0][0]      \n","__________________________________________________________________________________________________\n","flatten (Flatten)               (None, 20000)        0           activation_3[0][0]               \n","__________________________________________________________________________________________________\n","z_mean (Dense)                  (None, 20000)        400020000   flatten[0][0]                    \n","__________________________________________________________________________________________________\n","z_log_var (Dense)               (None, 20000)        400020000   flatten[0][0]                    \n","__________________________________________________________________________________________________\n","sampling (Sampling)             (None, 20000)        0           z_mean[0][0]                     \n","                                                                 z_log_var[0][0]                  \n","==================================================================================================\n","Total params: 800,082,948\n","Trainable params: 800,082,498\n","Non-trainable params: 450\n","__________________________________________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E7SCkjomASNP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1616107404118,"user_tz":0,"elapsed":4436,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}},"outputId":"f3fe01bf-67ff-4278-8a07-212e7c33bbde"},"source":["# OC-FakeDect Decoder\n","\n","latent_inputs = keras.Input(shape=(LATENT_DIM,))\n","# 20000\n","x = layers.Reshape((25, 25, 32))(latent_inputs)\n","x = layers.Conv2DTranspose(32, 3, strides=1, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 25x25x32\n","x = layers.Conv2DTranspose(64, 3, strides=2, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 50x50x64\n","x = layers.Conv2DTranspose(32, 3, strides=1, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 50x50x32\n","x = layers.Conv2DTranspose(16, 3, strides=2, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","x = layers.Activation('relu')(x)\n","# 100x100x16\n","x = layers.Conv2DTranspose(3, 3, strides=1, padding=\"same\")(x)\n","x = layers.BatchNormalization(axis = 1)(x)\n","decoder_outputs = layers.Activation('sigmoid')(x)\n","# 100x100x3\n","\n","# Defining full decoder as keras model\n","decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")\n","decoder.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"decoder\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 20000)]           0         \n","_________________________________________________________________\n","reshape (Reshape)            (None, 25, 25, 32)        0         \n","_________________________________________________________________\n","conv2d_transpose (Conv2DTran (None, 25, 25, 32)        9248      \n","_________________________________________________________________\n","batch_normalization_4 (Batch (None, 25, 25, 32)        100       \n","_________________________________________________________________\n","activation_4 (Activation)    (None, 25, 25, 32)        0         \n","_________________________________________________________________\n","conv2d_transpose_1 (Conv2DTr (None, 50, 50, 64)        18496     \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 50, 50, 64)        200       \n","_________________________________________________________________\n","activation_5 (Activation)    (None, 50, 50, 64)        0         \n","_________________________________________________________________\n","conv2d_transpose_2 (Conv2DTr (None, 50, 50, 32)        18464     \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 50, 50, 32)        200       \n","_________________________________________________________________\n","activation_6 (Activation)    (None, 50, 50, 32)        0         \n","_________________________________________________________________\n","conv2d_transpose_3 (Conv2DTr (None, 100, 100, 16)      4624      \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 100, 100, 16)      400       \n","_________________________________________________________________\n","activation_7 (Activation)    (None, 100, 100, 16)      0         \n","_________________________________________________________________\n","conv2d_transpose_4 (Conv2DTr (None, 100, 100, 3)       435       \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 100, 100, 3)       400       \n","_________________________________________________________________\n","activation_8 (Activation)    (None, 100, 100, 3)       0         \n","=================================================================\n","Total params: 52,567\n","Trainable params: 51,917\n","Non-trainable params: 650\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9i3HH_uZASNT","executionInfo":{"status":"ok","timestamp":1616107404318,"user_tz":0,"elapsed":4631,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}}},"source":["# Build VAE model as custom (keras model class) class\n","class VAE(keras.Model):\n","    def __init__(self, encoder, decoder, **kwargs):\n","        super(VAE, self).__init__(**kwargs)\n","        self.encoder = encoder\n","        self.decoder = decoder\n","        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n","        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n","        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n","\n","    @property\n","    def metrics(self):\n","        return [self.total_loss_tracker,\n","                self.reconstruction_loss_tracker,\n","                self.kl_loss_tracker]\n","\n","    def call(self, inputs):\n","        x = inputs[0]\n","        X = np.expand_dims(x, axis=0)\n","        z_mean, z_log_var, z = encoder(X)\n","        reconstruction = decoder(z)\n","        z_mean, z_log_var, z = encoder(X)\n","        reconstruction = decoder(z)\n","        reconstruction_loss = tf.reduce_mean(keras.losses.binary_crossentropy(inputs, reconstruction))\n","        reconstruction_loss *= 28 * 28\n","        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n","        kl_loss = tf.reduce_mean(kl_loss)\n","        kl_loss *= -0.5\n","        total_loss = reconstruction_loss + kl_loss\n","        self.add_metric(kl_loss, name='kl_loss', aggregation='mean')\n","        self.add_metric(total_loss, name='total_loss', aggregation='mean')\n","        self.add_metric(reconstruction_loss, name='reconstruction_loss', aggregation='mean')\n","        return reconstruction\n","\n","    def train_step(self, data):\n","        with tf.GradientTape() as tape:\n","            z_mean, z_log_var, z = self.encoder(data)\n","            reconstruction = self.decoder(z)\n","            \n","            reconstruction_loss = tf.reduce_mean(\n","                tf.reduce_sum(binary_crossentropy(data, reconstruction),\n","                              axis=(1, 2)))\n","            \n","            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n","            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n","            total_loss = reconstruction_loss + kl_loss\n","            \n","        grads = tape.gradient(total_loss, self.trainable_weights)\n","        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n","        self.total_loss_tracker.update_state(total_loss)\n","        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n","        self.kl_loss_tracker.update_state(kl_loss)\n","        \n","        return {\"loss\": self.total_loss_tracker.result(),\n","                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n","                \"kl_loss\": self.kl_loss_tracker.result()}\n","\n","    def test_step(self, data):\n","        if isinstance(data, tuple):\n","            data = data[0]\n","\n","        z_mean, z_log_var, z = encoder(data)\n","        reconstruction = decoder(z)\n","        reconstruction_loss = tf.reduce_mean(\n","            keras.losses.binary_crossentropy(data, reconstruction)\n","        )\n","        reconstruction_loss *= 28 * 28\n","        kl_loss = 1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n","        kl_loss = tf.reduce_mean(kl_loss)\n","        kl_loss *= -0.5\n","        total_loss = reconstruction_loss + kl_loss\n","        return {\"loss\": total_loss,\n","                \"reconstruction_loss\": reconstruction_loss,\n","                \"kl_loss\": kl_loss}"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"kTUOyg8zWkg8","executionInfo":{"status":"ok","timestamp":1616107404320,"user_tz":0,"elapsed":4628,"user":{"displayName":"Sakib Ahamed","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghux0A3tJ8ak2hXsUY0-OJKE1g54h6Vq61r4o_n9GY=s64","userId":"01642899865297704166"}}},"source":["# Define callbacks e.g. Early Stopping\n","EARLY_STOP = EarlyStopping(monitor='reconstruction_loss',\n","                           patience=1,\n","                           mode='min',\n","                           verbose=1,\n","                           restore_best_weights=True)\n","\n","# Define Model\n","vae = VAE(encoder, decoder)\n","vae.compile(optimizer=keras.optimizers.Adam())"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iLYtSY4bnlhA","outputId":"ddb82472-7b8b-454b-f596-ade185d324fc"},"source":["# Trains for epochs, also very slow\n","for _ in tqdm(range(EPOCHS)):\n","    for _ in tqdm(range(TRAIN_GENERATOR.n//BATCH_SIZE+1)):\n","        vae.fit(np.concatenate([TRAIN_GENERATOR.next()[0], VALIDATION_GENERATOR.next()[0]], axis=0),\n","                epochs=100,\n","                batch_size=2*BATCH_SIZE,\n","                verbose=1,\n","                callbacks=[EARLY_STOP])\n","    # vae.save_weights('./Checkpoints/OC-FakeDect/')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  0%|          | 0/1 [00:00<?, ?it/s]\n","  0%|          | 0/27 [00:00<?, ?it/s]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/100\n","1/1 [==============================] - 11s 11s/step - loss: 6928.9902 - reconstruction_loss: 6919.6655 - kl_loss: 9.3248\n","Epoch 2/100\n","1/1 [==============================] - 1s 561ms/step - loss: 6914.2617 - reconstruction_loss: 6865.1274 - kl_loss: 49.1341\n","Epoch 3/100\n","1/1 [==============================] - 1s 562ms/step - loss: 6810.4653 - reconstruction_loss: 6809.5688 - kl_loss: 0.8967\n","Epoch 4/100\n","1/1 [==============================] - 1s 562ms/step - loss: 6748.2773 - reconstruction_loss: 6748.2583 - kl_loss: 0.0192\n","Epoch 5/100\n","1/1 [==============================] - 1s 559ms/step - loss: 6684.2803 - reconstruction_loss: 6684.2578 - kl_loss: 0.0223\n","Epoch 6/100\n","1/1 [==============================] - 1s 555ms/step - loss: 6629.2915 - reconstruction_loss: 6629.2539 - kl_loss: 0.0374\n","Epoch 7/100\n","1/1 [==============================] - 1s 560ms/step - loss: 6599.9131 - reconstruction_loss: 6599.7476 - kl_loss: 0.1656\n","Epoch 8/100\n","1/1 [==============================] - 1s 561ms/step - loss: 6591.9302 - reconstruction_loss: 6590.7983 - kl_loss: 1.1320\n","Epoch 9/100\n","1/1 [==============================] - 1s 563ms/step - loss: 6571.4683 - reconstruction_loss: 6565.2427 - kl_loss: 6.2254\n","Epoch 10/100\n","1/1 [==============================] - 1s 558ms/step - loss: 6542.4609 - reconstruction_loss: 6522.8726 - kl_loss: 19.5885\n","Epoch 11/100\n","1/1 [==============================] - 1s 557ms/step - loss: 6511.1616 - reconstruction_loss: 6498.5952 - kl_loss: 12.5662\n","Epoch 12/100\n","1/1 [==============================] - 1s 559ms/step - loss: 6487.1230 - reconstruction_loss: 6480.6929 - kl_loss: 6.4301\n","Epoch 13/100\n","1/1 [==============================] - 1s 564ms/step - loss: 6470.7476 - reconstruction_loss: 6466.8979 - kl_loss: 3.8497\n","Epoch 14/100\n","1/1 [==============================] - 1s 554ms/step - loss: 6461.8994 - reconstruction_loss: 6458.9409 - kl_loss: 2.9586\n","Epoch 15/100\n","1/1 [==============================] - 1s 567ms/step - loss: 6457.7842 - reconstruction_loss: 6454.7661 - kl_loss: 3.0181\n","Epoch 16/100\n","1/1 [==============================] - 1s 567ms/step - loss: 6455.4473 - reconstruction_loss: 6451.6367 - kl_loss: 3.8105\n","Epoch 17/100\n","1/1 [==============================] - 1s 568ms/step - loss: 6453.9336 - reconstruction_loss: 6448.5903 - kl_loss: 5.3433\n","Epoch 18/100\n","1/1 [==============================] - 1s 560ms/step - loss: 6451.7700 - reconstruction_loss: 6444.1855 - kl_loss: 7.5845\n","Epoch 19/100\n","1/1 [==============================] - 1s 556ms/step - loss: 6446.1230 - reconstruction_loss: 6435.7173 - kl_loss: 10.4059\n","Epoch 20/100\n","1/1 [==============================] - 1s 567ms/step - loss: 6441.1675 - reconstruction_loss: 6427.3433 - kl_loss: 13.8243\n","Epoch 21/100\n","1/1 [==============================] - 1s 552ms/step - loss: 6434.6582 - reconstruction_loss: 6416.5171 - kl_loss: 18.1411\n","Epoch 22/100\n","1/1 [==============================] - 1s 565ms/step - loss: 6427.1426 - reconstruction_loss: 6401.6548 - kl_loss: 25.4877\n","Epoch 23/100\n","1/1 [==============================] - 1s 568ms/step - loss: 6426.1636 - reconstruction_loss: 6397.9141 - kl_loss: 28.2494\n","Epoch 24/100\n","1/1 [==============================] - 1s 568ms/step - loss: 6421.1353 - reconstruction_loss: 6375.0405 - kl_loss: 46.0948\n","Epoch 25/100\n","1/1 [==============================] - 1s 563ms/step - loss: 6404.6582 - reconstruction_loss: 6372.5308 - kl_loss: 32.1273\n","Epoch 26/100\n","1/1 [==============================] - 1s 559ms/step - loss: 6403.2002 - reconstruction_loss: 6372.5571 - kl_loss: 30.6429\n","Restoring model weights from the end of the best epoch.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  4%|▎         | 1/27 [01:37<42:18, 97.64s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 00026: early stopping\n","Epoch 1/100\n","1/1 [==============================] - 1s 615ms/step - loss: 6379.9492 - reconstruction_loss: 6350.4458 - kl_loss: 29.5037\n","Epoch 2/100\n","1/1 [==============================] - 1s 566ms/step - loss: 6383.1426 - reconstruction_loss: 6325.8047 - kl_loss: 57.3377\n","Epoch 3/100\n","1/1 [==============================] - 1s 561ms/step - loss: 6363.7876 - reconstruction_loss: 6317.8530 - kl_loss: 45.9347\n","Epoch 4/100\n","1/1 [==============================] - 1s 561ms/step - loss: 6360.4683 - reconstruction_loss: 6327.6226 - kl_loss: 32.8456\n","Restoring model weights from the end of the best epoch.\n"],"name":"stdout"},{"output_type":"stream","text":["\n","  7%|▋         | 2/27 [02:25<34:25, 82.64s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 00004: early stopping\n","Epoch 1/100\n","1/1 [==============================] - 2s 2s/step - loss: 6430.1172 - reconstruction_loss: 6394.2324 - kl_loss: 35.8846\n","Epoch 2/100\n","1/1 [==============================] - 1s 548ms/step - loss: 6412.1865 - reconstruction_loss: 6355.8770 - kl_loss: 56.3097\n","Epoch 3/100\n","1/1 [==============================] - 1s 556ms/step - loss: 6414.1631 - reconstruction_loss: 6335.9053 - kl_loss: 78.2579\n","Epoch 4/100\n","1/1 [==============================] - 1s 539ms/step - loss: 6394.1807 - reconstruction_loss: 6335.9150 - kl_loss: 58.2658\n","Restoring model weights from the end of the best epoch.\n"],"name":"stdout"},{"output_type":"stream","text":["\n"," 11%|█         | 3/27 [02:38<24:46, 61.95s/it]\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["Epoch 00004: early stopping\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"igjOMUGwkdFs"},"source":["# Plot 5 images from test set\n","(x_train, _), (x_test, _) = TRAIN_GENERATOR.next(), VALIDATION_GENERATOR.next()\n","\n","for x in x_train[-3:]:\n","    X = np.expand_dims(x, axis=0)\n","    Z_mean, Z_log_var, Z = vae.encoder.predict(X)\n","    X_prime = vae.decoder.predict(Z)\n","    face = np.array(X_prime.reshape(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)*255, dtype=np.uint8)\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n","    axes[0].imshow(X.squeeze())\n","    axes[1].imshow(face)\n","    fig.tight_layout()\n","\n","for x in x_test[-3:]:\n","    X = np.expand_dims(x, axis=0)\n","    Z_mean, Z_log_var, Z = vae.encoder.predict(X)\n","    X_prime = vae.decoder.predict(Z)\n","    face = np.array(X_prime.reshape(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS)*255, dtype=np.uint8)\n","    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(5, 5))\n","    axes[0].imshow(X.squeeze())\n","    axes[1].imshow(face)\n","    fig.tight_layout()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HLE-Ww4Y37x2"},"source":["# # Load all training data, takes a very long time (3hrs)\n","# training_data = []\n","# for i in tqdm(range(TRAIN_GENERATOR.n//BATCH_SIZE+1)):\n","#     (x_train, _), (x_test, _) = TRAIN_GENERATOR.next(), VALIDATION_GENERATOR.next()\n","#     training_data.extend(np.concatenate([x_train, x_test], axis=0)) \n","\n","# vae.fit(training_data,\n","#         epochs=100,\n","#         batch_size=2*BATCH_SIZE,\n","#         verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-lvQuWrXTZ09"},"source":["# # Trains model for 1 step (i.e. one batch, not an epoch)\n","# (x_train, _), (x_test, _) = TRAIN_GENERATOR.next(), VALIDATION_GENERATOR.next()\n","# training_data = np.concatenate([x_train, x_test], axis=0)\n","# vae.fit(training_data,\n","#         epochs=100,\n","#         batch_size=2*BATCH_SIZE,\n","#         verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kgT50NRNO4Lu"},"source":["# OLD Autoencoder (for use w/ 28x28 images, used for testing)\n","\n","# encoder_inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n","# x = layers.Convolution2D(64, (3, 3), padding = 'same')(encoder_inputs)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Convolution2D(64, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.AveragePooling2D()(x)\n","# x = layers.Convolution2D(128, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Convolution2D(128, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.AveragePooling2D()(x)\n","# x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Conv2D(64, 3, strides=2, padding=\"same\")(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Conv2D(128, 3, strides=2, padding=\"same\")(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Flatten()(x)\n","# x = layers.Dense(128)(x)\n","# x = layers.BatchNormalization(axis = 1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Dense(128)(x)\n","# x = layers.Activation('tanh')(x)\n","# z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n","# z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n","# z = Sampling()([z_mean, z_log_var])\n","# encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")\n","# latent_inputs = keras.Input(shape=(LATENT_DIM,))\n","# x = layers.Dense(49)(latent_inputs)\n","# x = layers.BatchNormalization()(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Reshape((7, 7, 1))(x)\n","# x = layers.Convolution2D(128, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis=1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Convolution2D(128, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis=1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.UpSampling2D()(x)\n","# x = layers.Convolution2D(64, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis=1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.Convolution2D(64, (3, 3), padding = 'same')(x)\n","# x = layers.BatchNormalization(axis=1)(x)\n","# x = layers.Activation('relu')(x)\n","# x = layers.UpSampling2D()(x)\n","# decoder_outputs = layers.Convolution2D(3, (3, 3), padding = 'same', activation = 'tanh')(x)\n","# decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hxE1t2hITkga"},"source":["# (x_train, _) = TRAIN_GENERATOR.next()\n","# (x_test, _) = VALIDATION_GENERATOR.next()\n","# training_data = np.concatenate([x_train, x_test], axis=0)\n","# training_data = np.expand_dims(training_data, -1).astype(\"float32\")\n","# training_data = training_data[:, :, :, :, 0]"],"execution_count":null,"outputs":[]}]}