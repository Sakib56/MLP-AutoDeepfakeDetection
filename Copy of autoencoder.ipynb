{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Google Colab use\n",
    "\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    %cd '/content/drive/MyDrive/Colab Notebooks/DeepFake Detector'    \n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras-tuner in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (1.0.2)\n",
      "Requirement already satisfied: terminaltables in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (3.1.0)\n",
      "Requirement already satisfied: future in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (0.18.2)\n",
      "Requirement already satisfied: requests in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (2.25.1)\n",
      "Requirement already satisfied: scikit-learn in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (0.24.1)\n",
      "Requirement already satisfied: scipy in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (1.6.0)\n",
      "Requirement already satisfied: tqdm in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (4.56.0)\n",
      "Requirement already satisfied: tabulate in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (0.8.9)\n",
      "Requirement already satisfied: colorama in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (0.4.4)\n",
      "Requirement already satisfied: numpy in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (1.19.5)\n",
      "Requirement already satisfied: packaging in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from keras-tuner) (20.8)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from packaging->keras-tuner) (2.4.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from requests->keras-tuner) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from requests->keras-tuner) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from requests->keras-tuner) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from requests->keras-tuner) (1.26.2)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from scikit-learn->keras-tuner) (1.0.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/sakib/miniconda3/envs/sak/lib/python3.8/site-packages (from scikit-learn->keras-tuner) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "from __future__ import division\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from numpy.random import seed\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras\n",
    "from keras import preprocessing\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import layers, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "from keras.optimizers import *\n",
    "from keras.applications import *\n",
    "from keras import metrics\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "!pip install -U keras-tuner\n",
    "from kerastuner.tuners import RandomSearch, Hyperband\n",
    "from kerastuner.engine.hypermodel import HyperModel\n",
    "from kerastuner.engine.hyperparameters import HyperParameters\n",
    "from kerastuner import Objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorflow_version` not found.\n"
     ]
    }
   ],
   "source": [
    "# Check GPU available\n",
    "\n",
    "%tensorflow_version 2.x\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General model settings\n",
    "\n",
    "IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS = 28, 28, 3\n",
    "EPOCHS = 30\n",
    "DATA_GENERATOR_SEED = 1337\n",
    "BACTH_SIZE = 64\n",
    "VALIDATION_SPLIT = 0.2\n",
    "# LEARNING_RATE = 1e-3\n",
    "LATENT_DIM = 2\n",
    "\n",
    "tf.random.set_seed(DATA_GENERATOR_SEED)\n",
    "seed(DATA_GENERATOR_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random sampling function for latent vector, z; \"variational part\"\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Uses (z_mean, z_log_var) to sample z, the vector encoding a digit.\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\", strides=2, padding=\"same\")(encoder_inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(16, activation=\"relu\")(x)\n",
    "\n",
    "# Sampling\n",
    "\n",
    "z_mean = layers.Dense(LATENT_DIM, name=\"z_mean\")(x)\n",
    "z_log_var = layers.Dense(LATENT_DIM, name=\"z_log_var\")(x)\n",
    "z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "encoder = keras.Model(encoder_inputs, [z_mean, z_log_var, z], name=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "\n",
    "latent_inputs = keras.Input(shape=(LATENT_DIM,))\n",
    "\n",
    "x = layers.Dense(7 * 7 * 64, activation=\"relu\")(latent_inputs)\n",
    "x = layers.Reshape((7, 7, 64))(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\", strides=2, padding=\"same\")(x)\n",
    "\n",
    "decoder_outputs = layers.Conv2DTranspose(1, 3, activation=\"sigmoid\", padding=\"same\")(x)\n",
    "decoder = keras.Model(latent_inputs, decoder_outputs, name=\"decoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 28, 28, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 14, 14, 32)   896         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 64)     18496       conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 3136)         0           conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 16)           50192       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "z_mean (Dense)                  (None, 2)            34          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "z_log_var (Dense)               (None, 2)            34          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "sampling (Sampling)             (None, 2)            0           z_mean[0][0]                     \n",
      "                                                                 z_log_var[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 69,652\n",
      "Trainable params: 69,652\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"decoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 2)]               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3136)              9408      \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 14, 14, 64)        36928     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 28, 28, 32)        18464     \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_2 (Conv2DTr (None, 28, 28, 1)         289       \n",
      "=================================================================\n",
      "Total params: 65,089\n",
      "Trainable params: 65,089\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# VAE summary\n",
    "\n",
    "encoder.summary()\n",
    "decoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build VAE model\n",
    "\n",
    "class VAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(VAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.total_loss_tracker,\n",
    "                self.reconstruction_loss_tracker,\n",
    "                self.kl_loss_tracker]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(data)\n",
    "            reconstruction = self.decoder(z)\n",
    "            \n",
    "            reconstruction_loss = tf.reduce_mean(\n",
    "                tf.reduce_sum(binary_crossentropy(data, reconstruction),\n",
    "                              axis=(1, 2)))\n",
    "            \n",
    "            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n",
    "            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "            \n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "        \n",
    "        return {\"loss\": self.total_loss_tracker.result(),\n",
    "                \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n",
    "                \"kl_loss\": self.kl_loss_tracker.result()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick dataset; DF_TYPE={'frames', 'avg'}\n",
    "\n",
    "DF_TYPE = 'frames'\n",
    "\n",
    "TRAIN_VAL_DIR = f'./Celeb-DF-v2/Celeb-{DF_TYPE}'\n",
    "if 'bal' in TRAIN_VAL_DIR:\n",
    "    TEST_DIR = TRAIN_VAL_DIR.replace('bal', 'test')\n",
    "else:\n",
    "    TEST_DIR = f'{TRAIN_VAL_DIR}-test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 images belonging to 2 classes.\n",
      "Found 0 images belonging to 2 classes.\n",
      "Found 0 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATAGEN = ImageDataGenerator(rescale = 1./255.,\n",
    "                                   rotation_range = 20,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.15,\n",
    "                                   zoom_range = 0.15,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode='nearest',\n",
    "                                   validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "VAL_DATAGEN = ImageDataGenerator(rescale = 1.0/255., \n",
    "                                 validation_split = VALIDATION_SPLIT)\n",
    "\n",
    "TEST_DATAGEN = ImageDataGenerator(rescale = 1.0/255.)\n",
    "\n",
    "TRAIN_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n",
    "                                                    batch_size = BACTH_SIZE,\n",
    "                                                    class_mode = 'binary', \n",
    "                                                    target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                    subset = 'training',\n",
    "                                                    seed = DATA_GENERATOR_SEED)\n",
    "\n",
    "VALIDATION_GENERATOR = TRAIN_DATAGEN.flow_from_directory(directory = TRAIN_VAL_DIR,\n",
    "                                                         batch_size = BACTH_SIZE,\n",
    "                                                         class_mode = 'binary', \n",
    "                                                         target_size = (IMG_HEIGHT, IMG_WIDTH),\n",
    "                                                         subset = 'validation',\n",
    "                                                         seed = DATA_GENERATOR_SEED)\n",
    "\n",
    "TEST_GENERATOR = TEST_DATAGEN.flow_from_directory(directory = TEST_DIR,\n",
    "                                                  batch_size = BACTH_SIZE,\n",
    "                                                  class_mode = 'binary', \n",
    "                                                  target_size = (IMG_HEIGHT, IMG_WIDTH),                                \n",
    "                                                  seed = DATA_GENERATOR_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _) = TRAIN_GENERATOR.next()\n",
    "(x_test, _) = VALIDATION_GENERATOR.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "reals = np.concatenate([x_train, x_test], axis=0)\n",
    "reals = np.expand_dims(reals, -1).astype(\"float32\") / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(encoder, decoder)\n",
    "vae.compile(optimizer=keras.optimizers.Adam())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.fit(reals, \n",
    "        epochs=30, \n",
    "        batch_size=128)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
